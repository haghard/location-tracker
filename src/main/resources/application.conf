akka.http.server.preview.enable-http2 = on

orders-service {
  ask-timeout = 2 s

  #grpc {
  # consider setting this to a specific interface for your environment
  #  interface = "0.0.0.0"
  #  port = 8101
  #  port = ${?GRPC_PORT}
  #}
}


akka {
  loglevel = DEBUG
  # Optimization: If an event would not be logged by slf4j, it will not be sent to the logging event bus.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  # Only use the slf4j logging, do not log directly to console.
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  stdout-loglevel = OFF
  logger-startup-timeout = 60s


  log-dead-letters = 10
  log-dead-letters-during-shutdown = false


  actor {

    allow-java-serialization = on #off
    warn-about-java-serializer-usage = on

    serializers {
      #akka-replicated-data  = "akka.cluster.ddata.protobuf.ReplicatedDataSerializer"
      akka-replicated-data  = "akka.cluster.ddata.CRDTSerializer"

      #akka-data-replication = "akka.cluster.ddata.protobuf.ReplicatorMessageSerializer"
      akka-data-replication = "akka.cluster.ddata.CustomReplicatorMessageSerializerUdp"


      #akka-sharding = "akka.cluster.sharding.protobuf.ClusterShardingMessageSerializer"
      akka-sharding = "akka.cluster.sharding.CustomClusterShardingMessageSerializer"

      ser = com.rides.persistence.VehicleStateSerializer
    }

    serialization-identifiers {
      #"akka.cluster.ddata.protobuf.ReplicatorMessageSerializer" = 11
      "akka.cluster.ddata.CRDTSerializer" = 11

      "akka.cluster.ddata.CustomReplicatorMessageSerializerUdp" = 12

      #"akka.cluster.sharding.protobuf.ClusterShardingMessageSerializer" = 13
      "akka.cluster.sharding.CustomClusterShardingMessageSerializer" = 13
    }

    serialization-bindings {
      "com.rides.domain.types.protobuf.VehicleStatePB" = ser

      //cmd
      "com.rides.domain.ReportLocation" = ser
      "com.rides.domain.GetLocation" = ser
      //reply
      "com.rides.VehicleReply" = ser
    }

    provider = akka.cluster.ClusterActorRefProvider

    default-dispatcher {
      fork-join-executor {
        parallelism-min = 2
        parallelism-factor = 1.0
        parallelism-max = 6
      }
    }

    # Default separate internal dispatcher to run Akka internal tasks and actors on
    # protecting them against starvation because of accidental blocking in user actors (which run on the
    # default dispatcher)
    internal-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      throughput = 5
      fork-join-executor {
        parallelism-min = 2
        parallelism-factor = 1.0
        parallelism-max = 4
      }
    }
  }

  cluster {
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
    sharding {
      #https://doc.akka.io/docs/akka/current/typed/cluster-sharding.html?_ga=2.225099034.1493026331.1653164470-1899964194.1652800389#automatic-passivation
      passivation {
        strategy = default-strategy
        default-strategy {
          active-entity-limit = 1000
          idle-entity.timeout = 5 m  #  20 s
        }
      }
    }

    # akka://.../system/DDataStateActor
    distributed-data = {
      name = DDataStateActor
    }
  }

  # CoordinatedShutdown will run the tasks that are added to these
  # phases. The phases can be ordered as a DAG by defining the
  # dependencies between the phases.
  # Each phase is defined as a named config section with the
  # following optional properties:
  # - timeout=15s: Override the default-phase-timeout for this phase.
  # - recover=off: If the phase fails the shutdown is aborted
  #                and depending phases will not be executed.
  # depends-on=[]: Run the phase after the given phases
  coordinated-shutdown {

    phases {
      cluster-sharding-shutdown-region {
        timeout = 15 s  #10 s
        depends-on = [before-cluster-shutdown]
      }
    }

    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on
    default-phase-timeout = 10 seconds
  }

  //https://doc.akka.io/docs/akka-management/current/akka-management.html
  management {
    http {
      base-path = "rides"

      #port = 8558
      #bind-port = 8558
      #hostname
      #bind-hostname
      route-providers-read-only = false
    }

    cluster.bootstrap {
      # Cluster Bootstrap will always attempt to join an existing cluster if possible. However
      # if no contact point advertises any seed-nodes a new cluster will be formed by the
      # node with the lowest address as decided by [[LowestAddressJoinDecider]].
      # Setting `new-cluster-enabled=off` after an initial cluster has formed is recommended to prevent new clusters
      # forming during a network partition when nodes are redeployed or restarted.
      # Replaces `form-new-cluster`, if `form-new-cluster` is set it takes precedence over this
      # property for backward compatibility
      new-cluster-enabled = on

      contact-point-discovery {

        service-name = rides

        discovery-method = config

        port-name = akka.management.http.port #management

        # Interval at which service discovery will be polled in search for new contact-points
        interval = 1 second

        # The smallest number of contact points that need to be discovered before the bootstrap process can start.
        # For optimal safety during cluster formation, you may want to set these value to the number of initial
        # nodes that you know will participate in the cluster (e.g. the value of `spec.replicas` as set in your kubernetes config).
        required-contact-point-nr = 2

        # Amount of time for which a discovery observation must remain "stable"
        # (i.e. not change list of discovered contact-points) before a join decision can be made.
        # This is done to decrease the likelyhood of performing decisions on fluctuating observations.
        #
        # This timeout represents a tradeoff between safety and quickness of forming a new cluster.
        stable-margin = 3 seconds #5 seconds

        # Timeout for getting a reply from the service-discovery subsystem
        resolve-timeout = 3 seconds
      }
    }

    health-checks {
      readiness-path = "health/ready"
      liveness-path = "health/alive"
    }
  }



  remote {
    # Disable event logging
    log-remote-lifecycle-events = off
    artery {
      # Select the underlying transport implementation.
      # Possible values: aeron-udp, tcp, tls-tcp
      # See https://doc.akka.io/docs/akka/current/remoting-artery.html#selecting-a-transport for the tradeoffs
      # for each transport

      #https://doc.akka.io/docs/akka/current/remoting-artery.html#selecting-a-transport
      transport = aeron-udp
      #transport = tcp

      #https://doc.akka.io/docs/akka/current/remoting-artery.html#dedicated-subchannel-for-large-messages
      # Dedicated subchannel for large messages
      # /user/repl-supervisor/typed-replicator/underlying#-831406423

      # Actor paths to use the large message stream for when a message
      # is sent to them over remoting. The large message stream dedicated
      # is separate from "normal" and system messages so that sending a
      # large message does not interfere with them.
      # Entries should be the full path to the actor. Wildcards in the form of "*"
      # can be supplied at any place and matches any name at that segment -
      # "/user/supervisor/actor/*" will match any direct child to actor,
      # while "/supervisor/*/child" will match any grandchild to "supervisor" that
      # has the name "child"
      # Entries have to be specified on both the sending and receiving side.
      # Messages sent to ActorSelections will not be passed through the large message
      # stream, to pass such messages through the large message stream the selections
      # but must be resolved to ActorRefs first.

      # Without that
      # akka://fsa@127.0.0.2:2550/user/repl-supervisor/typed-replicator/underlying] akka.remote.RemoteActorRef -
      # Message [akka.cluster.ddata.Replicator$Internal$Gossip] from Actor[akka://fsa/user/repl-supervisor/typed-replicator/underlying#219204214]
      # to Actor[akka://fsa@127.0.0.2:2550/user/repl-supervisor/typed-replicator/underlying#-891320546] was dropped.
      # Discarding oversized payload sent to Some(Actor[akka://fsa@127.0.0.2:2550/user/repl-supervisor/typed-replicator/underlying#-891320546]):
      # max allowed size 159744 bytes. Message type [akka.cluster.ddata.Replicator$Internal$Gossip]..

      large-message-destinations = [
        #Is not used
        #"/system/clusterReceptionist/replicator"

        #default replicator for sharding
        #Stores
        # 1. akka.cluster.sharding.ShardCoordinator.Internal.State with grows with number of Shards|entities
        "/system/sharding/replicator"


        //Custom replicator for application ddata (MultiValRegister, Gossip, Status)
        //akka://rides/user/replicator
        "/user/replicator"
      ]

      # To notice large messages you can enable logging of message types with payload size in bytes larger than the configured
      log-frame-size-exceeding = 250 KiB

      advanced {
        # Maximum serialized message size, including header data.
        maximum-frame-size = 256 KiB #512 KiB

        # Direct byte buffers are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-frame-size'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        buffer-pool-size = 128

        # Maximum serialized message size for the large messages, including header data.
        # It is currently restricted to 1/8th the size of a term buffer that can be
        # configured by setting the 'aeron.term.buffer.length' system property.
        # See 'large-message-destinations'.
        maximum-large-frame-size = 2 MiB

        # Direct byte buffers for the large messages are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-large-frame-size'.
        # See 'large-message-destinations'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        large-buffer-pool-size = 32


        # Total number of inbound lanes, shared among all inbound associations. A value
        # greater than 1 means that deserialization can be performed in parallel for
        # different destination actors. The selection of lane is based on consistent
        # hashing of the recipient ActorRef to preserve message ordering per receiver.
        # Lowest latency can be achieved with inbound-lanes=1 because of one less
        # asynchronous boundary.
        inbound-lanes = 4

        # Number of outbound lanes for each outbound association. A value greater than 1
        # means that serialization and other work can be performed in parallel for different
        # destination actors. The selection of lane is based on consistent hashing of the
        # recipient ActorRef to preserve message ordering per receiver. Note that messages
        # for different destination systems (hosts) are handled by different streams also
        # when outbound-lanes=1. Lowest latency can be achieved with outbound-lanes=1
        # because of one less asynchronous boundary.
        outbound-lanes = 1


        # Only used when transport is aeron-udp
        aeron {

          # Periodically log out all Aeron counters. See https://github.com/real-logic/aeron/wiki/Monitoring-and-Debugging#counters
          # Only used when transport is aeron-udp.
          log-aeron-counters = false

          # Controls whether to start the Aeron media driver in the same JVM or use external
          # process. Set to 'off' when using external media driver, and then also set the
          # 'aeron-dir'.
          # Only used when transport is aeron-udp.
          embedded-media-driver = on

          # Directory used by the Aeron media driver. It's mandatory to define the 'aeron-dir'
          # if using external media driver, i.e. when 'embedded-media-driver = off'.
          # Embedded media driver will use a this directory, or a temporary directory if this
          # property is not defined (empty).
          # Only used when transport is aeron-udp.
          #aeron-dir = ""

          # Whether to delete aeron embedded driver directory upon driver stop.
          # Only used when transport is aeron-udp.
          delete-aeron-dir = yes

          # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
          # The tradeoff is that to have low latency more CPU time must be used to be
          # able to react quickly on incoming messages or send as fast as possible after
          # backoff backpressure.
          # Level 1 strongly prefer low CPU consumption over low latency.
          # Level 10 strongly prefer low latency over low CPU consumption.
          # Only used when transport is aeron-udp.
          idle-cpu-level = 1

          # messages that are not accepted by Aeron are dropped after retrying for this period
          # Only used when transport is aeron-udp.
          give-up-message-after = 60 seconds

          # Timeout after which aeron driver has not had keepalive messages
          # from a client before it considers the client dead.
          # Only used when transport is aeron-udp.
          client-liveness-timeout = 20 seconds

          # Timout after after which an uncommitted publication will be unblocked
          # Only used when transport is aeron-udp.
          publication-unblock-timeout = 40 seconds

          # Timeout for each the INACTIVE and LINGER stages an aeron image
          # will be retained for when it is no longer referenced.
          # This timeout must be less than the 'handshake-timeout'.
          # Only used when transport is aeron-udp.
          image-liveness-timeout = 10 seconds

          # Timeout after which the aeron driver is considered dead
          # if it does not update its C'n'C timestamp.
          # Only used when transport is aeron-udp.
          driver-timeout = 20 seconds
        }
      }
    }
  }

  persistence {
    #state.plugin = "jdbc-durable-state-store"
    #state.plugin = rocks-db

    #state.plugin = mmap-db
    state.plugin = mmap-db
  }
}


#rocks-db {
#  class = "com.rides.persistence.RocksDBProvider"
#  recovery-timeout = 10s
#}

#mmap-db {
#  class = "com.rides.persistence.MMapReplicatedStateProvider"
#  recovery-timeout = 10s
#}

mmap-db {
  class = "com.rides.persistence.ReplicatedMMapApi"
  recovery-timeout = 5s
}


slick {
  profile = "slick.jdbc.PostgresProfile$"
  db {
    host = "localhost"
    host = ${?DB_HOST}

    url = "jdbc:postgresql://"${slick.db.host}":5432/order_managment_db?reWriteBatchedInserts=true"
    #url = "jdbc:postgresql://127.0.0.1:5432/order_managment_db?reWriteBatchedInserts=true"
    user = "root"
    password = "secret"
    driver = "org.postgresql.Driver"
    numThreads = 5
    maxConnections = 5
    minConnections = 5
  }
}

jdbc-journal {
  slick = ${slick}
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  slick = ${slick}
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  slick = ${slick}
}

# the akka-persistence-jdbc provider in use for durable state store
jdbc-durable-state-store {
  #class = akka.persistence.jdbc.state.JdbcDurableStateStoreProvider
  slick = ${slick}
}

#include "postgres.conf"
#https://github.com/akka/akka-persistence-jdbc/blob/master/core/src/main/resources/reference.conf


disp {
  replicator-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    throughput = 1
    fork-join-executor {
      parallelism-min = 1
      parallelism-max = 2
    }
  }
}


app.replicator.distributed-data {

  use-dispatcher = disp.replicator-dispatcher

  # How often the Replicator checks for pruning of data associated with
  # removed cluster nodes.
  pruning-interval = 40 s #120 s

  # How long time it takes to spread the data to all other replica nodes.
  # This is used when initiating and completing the pruning process of data associated
  # with removed cluster nodes. The time measurement is stopped when any replica is
  # unreachable, but it's still recommended to configure this with certain margin.
  # It should be in the magnitude of minutes even though typical dissemination time
  # is shorter (grows logarithmic with number of nodes). There is no advantage of
  # setting this too low. Setting it to large value will delay the pruning process.

  # One thing that can be problematic with CRDTs is that some data types accumulate history (garbage).
  # For example a `GCounter` keeps track of one counter per node. If a `GCounter` has been updated
  # from one node it will associate the identifier of that node forever. That can become a problem
  # for long running systems with many cluster nodes being added and removed. To solve this problem
  # the `Replicator` performs pruning of data associated with nodes that have been removed from the
  # cluster. Data types that need pruning have to implement [[RemovedNodePruning]].
  # The pruning consists of several steps:
  #
  # When a node is removed from the cluster it is first important that all updates that were
  # done by that node are disseminated to all other nodes. The pruning will not start before the
  # `maxPruningDissemination` duration has elapsed. The time measurement is stopped when any
  # replica is unreachable, but it's still recommended to configure this with certain margin.
  # It should be in the magnitude of minutes.
  #
  # The nodes are ordered by their address and the node ordered first is called leader.
  # The leader initiates the pruning by adding a `PruningInitialized` marker in the data envelope.
  # This is gossiped to all other nodes and they mark it as seen when they receive it.
  # When the leader sees that all other nodes have seen the `PruningInitialized` marker
  # the leader performs the pruning and changes the marker to `PruningPerformed` so that nobody
  # else will redo the pruning. The data envelope with this pruning state is a CRDT itself.
  # The pruning is typically performed by "moving" the part of the data associated with
  # the removed node to the leader node. For example, a `GCounter` is a `Map` with the node as key
  # and the counts done by that node as value. When pruning the value of the removed node is
  # moved to the entry owned by the leader node. See [[RemovedNodePruning#prune]].</li>
  # Thereafter the data is always cleared from parts associated with the removed node so that
  # it does not come back when merging. See [[RemovedNodePruning#pruningCleanup]]</li>
  # <li>After another `maxPruningDissemination` duration after pruning the last entry from the
  # removed node the `PruningPerformed` markers in the data envelope are collapsed into a
  # single tombstone entry, for efficiency. Clients may continue to use old data and therefore
  # all data are always cleared from parts associated with tombstoned nodes.
  max-pruning-dissemination = 30 s

  # The markers of that pruning has been performed for a removed node are kept for this time and thereafter removed.
  # If old data entry that was never pruned is somehow injected and merged with existing data after this time the value will not be correct.
  # This would be possible (although unlikely) in the case of a long network partition. It should be in the magnitude of hours.

  # If a member stayed down for longer then `pruning-marker-time-to-live` and later brought back, it might bring back pruned data.
  pruning-marker-time-to-live = 2 m  #6 h

  durable.keys = []

  # Maximum number of entries to transfer in one round of gossip exchange when
  # synchronizing the replicas. Next chunk will be transferred in next round of gossip.
  # The actual number of data entries in each Gossip message is dynamically
  # adjusted to not exceed the maximum remote message size (maximum-frame-size).
  max-delta-elements = 64 #256

  #How often we try to repair data (send Status and recieve Gossip). Takes max-delta-elements elements for each round of gossip
  gossip-interval = 7 s #10 s
}
